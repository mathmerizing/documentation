

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Basics of Deep Learning &mdash; Autoencoder 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/doctools.js"></script>
        <script src="static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-Layer Perceptron: Classification of handwritten digits (MNIST)" href="classifier_demo.html" />
    <link rel="prev" title="Getting started: Overview &amp; Installation" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Autoencoder
          

          
            
            <img src="static/MLP.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting started: Overview &amp; Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Basics of Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#activation-functions">Activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#identity">Identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sigmoid">Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanh">Tanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relu">ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#leaky-relu">Leaky ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax">Softmax</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mean-squared-error">Mean Squared Error</a></li>
<li class="toctree-l3"><a class="reference internal" href="#crossentropy">Crossentropy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimizers">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#code-training-a-classifier">Code: Training a Classifier</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="classifier_demo.html">Multi-Layer Perceptron: Classification of handwritten digits (MNIST)</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoencoder_demo.html">Denoising Autoencoder: Removing noise from the MNIST dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">NumNeurNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="help.html">Help</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Autoencoder</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Basics of Deep Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="sources/basics.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="basics-of-deep-learning">
<h1>Basics of Deep Learning<a class="headerlink" href="#basics-of-deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Before we dive into some more advanced concepts in deep learning, let us recapitulate
the fundamentals. Deep learning is a subset of machine learning, in which neural networks,
which we will introduce shortly are trying to learn a mapping between samples from a dataset
and their labels. In the 20th century researchers, inspired by how the human brain works,
invented the <strong>Artificial Neural Network</strong> (ANN), whose architecture is depicted below.
Frequently we also refer to this datastructure as a <strong>Multi-Layer Perceptron</strong> (MLP).</p>
<div class="figure align-center" id="id1">
<img alt="multi_layer_perceptron" src="images/MLP.svg" /><p class="caption"><span class="caption-text">Figure 1: Architecture of a Multi-Layer Perceptron</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>To create this particular network you need to execute this code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nn</span> <span class="kn">import</span> <span class="n">MLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>As you can see, you simply need to create a new MLP object and add the two dense layers
by specifying the number of neurons in the input layer and the number of neurons in
the output layer.</p>
<p>How can we compute the prediction of the neural network, given some sample
from the dataset? In the following, we will answer this question for the case,
in which subsequent layers of neurons are fully connected. These are the so called <strong>dense layers</strong>.
For more sophisticated architectures and layer types, please go to the corresponding
chapters.</p>
<p>In the simplest, case a Multi-Layer Perceptron only has <strong>weights</strong> and <strong>biases</strong>
for each dense layer. Let us number these parameters from left to right,
i.e. <span class="math notranslate nohighlight">\(W^1\)</span> (resp. <span class="math notranslate nohighlight">\(b^1\)</span>) denotes the weights (biases)
between the red and the blue layer and <span class="math notranslate nohighlight">\(W^2\)</span> (resp. <span class="math notranslate nohighlight">\(b^2\)</span>) denotes the weights (biases)
between the blue and the green layer. Then the prediction of the neural network is given
by</p>
<div class="math notranslate nohighlight">
\[W^2\left(W^1x + b^1\right) + b^2\]</div>
<p>for some input vector <span class="math notranslate nohighlight">\(x\)</span>, since the i.th dense layer acts on an input vector <span class="math notranslate nohighlight">\(x^i\)</span> by</p>
<div class="math notranslate nohighlight">
\[W^ix^i + b^i\]</div>
<p>and we have <span class="math notranslate nohighlight">\(x^1 := x\)</span>. The process of evaluating the prediction of a neural network,
is called <strong>forward propagation</strong>, since an input vector is propagated through all
subsequent layers of a neural network.</p>
</div>
<div class="section" id="activation-functions">
<h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>In general, the dense layers are not sufficient the way that they have been presented above,
since they don’t contain any nonlinearities. Thus various <strong>activation functions σ</strong>
have been introduced, which are applied to the output of a given dense layer, i.e.
the i.th dense layer acts on an input vector <span class="math notranslate nohighlight">\(x^i\)</span> by</p>
<div class="math notranslate nohighlight">
\[\sigma\left( W^ix^i + b^i \right).\]</div>
<p>Which activation functions are there and which are suitable for my problem statement?
There is a vast array of activation functions which have been developed by researchers,
but we will only present you some of the most prominent examples, which have been also
incorporated into our framework. The choice of activation function is somewhat of an art itself,
but we would advise to try out the Sigmoid or ReLU function at first, since they have
proven to be quite useful in practice.</p>
<p>In our framework we have implemented the functions:</p>
<div class="section" id="identity">
<h3>Identity<a class="headerlink" href="#identity" title="Permalink to this headline">¶</a></h3>
<p>The identity function <span class="math notranslate nohighlight">\(f(x) = x\)</span> is rarely used in practice, since it is linear.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="sigmoid">
<h3>Sigmoid<a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<p>The sigmoid function has been inspired by the neurons in our brain:</p>
<div class="math notranslate nohighlight">
\[\sigma(x) := \frac{1}{1 + e^{-x}}.\]</div>
<p>Neurons in the brain are either active, i.e. they fire, which is represented by a 1,
or they are inactive, which is represented by a 0. Thus, the sigmoid function tries
to emulate the function of a brain’s neuron, since for positive inputs the function’s value is close to 1
and for negative inputs it is close to 0.</p>
<div class="figure align-center" id="id2">
<img alt="sigmoid" src="images/Sigmoid.svg" /><p class="caption"><span class="caption-text">Figure 2: Sigmoid function</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>Why don’t we use a step function, like the Heaviside function <span class="math notranslate nohighlight">\(f\)</span> with</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) :=
\begin{cases}
   0 &amp; \text{for } x \leq 0 \\
   1 &amp; \text{for } x  &gt; 0\\
\end{cases} \quad ?\end{split}\]</div>
<p>Firstly, this function doesn’t have a derivative in the strong sense.
Secondly, the variational derivative is 0 for <span class="math notranslate nohighlight">\(\mathbb{R} \setminus \{ 0\}\)</span>,
which would prevent the learning of parameters when applying backpropagation.
Hence we use the sigmoid function instead.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">Sigmoid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="tanh">
<h3>Tanh<a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<p>Similar arguments to the ones presented above can be used to motivate the usage
of the hyperbolic tangent</p>
<div class="math notranslate nohighlight">
\[\tanh(x) := \frac{\sinh(x)}{\cosh(x)},\]</div>
<p>where <span class="math notranslate nohighlight">\(\sinh(x) = \frac{e^x - e^{-x}}{2}\)</span> is the hyperbolic sine
and <span class="math notranslate nohighlight">\(\cosh(x) = \frac{e^x + e^{-x}}{2}\)</span> is the hyperbolic cosine.</p>
<div class="figure align-center" id="id3">
<img alt="tanh" src="images/Tanh.svg" /><p class="caption"><span class="caption-text">Figure 3: Hyperbolic tangent</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>A major difference between the sigmoid and the tanh function is that for <span class="math notranslate nohighlight">\(x \leq 0\)</span> the
activation function is now approximately -1. The hyperbolic tangent is used for example
for LSTMs, which are a type of Recurrent Neural Network (RNN).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">Tanh</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="relu">
<h3>ReLU<a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<p>Although the previous functions were continously differentiable, they were costly
to compute, since sigmoid and hyperbolic tangent require the evaluation of the
exponential function. Thus data scientist choose to work with the Rectified Linear Unit (ReLU)</p>
<div class="math notranslate nohighlight">
\[\text{relu}(x) := \max(0,x)\]</div>
<p>and its variations very often.</p>
<div class="figure align-center" id="id4">
<img alt="relu" src="images/ReLU.svg" /><p class="caption"><span class="caption-text">Figure 4: Rectified Linear Unit</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Unfortunately the disadvantages of this simple activation function are that the function
is not continously differentiable at 0 and that the slope of the function vanishes
for <span class="math notranslate nohighlight">\(x &lt; 0\)</span>. These disadvantages can be eliminated by using the exponential
linear unit (ELU) with</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) :=
\begin{cases}
   x &amp; \text{for } x \geq 0 \\
   e^x - 1 &amp; \text{for } x  &lt; 0\\
\end{cases} \quad .\end{split}\]</div>
<p>However, we haven’t implemented this function in our framework, since it also requires
the evaluation of the exponential function. If we don’t want the gradient to vanish
for negative inputs, we can alternatively use Leaky ReLU.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">ReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="leaky-relu">
<h3>Leaky ReLU<a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h3>
<p>The Leaky Rectified Linear Unit (Leaky ReLU) differs from ReLU only by its function values for negative inputs.</p>
<div class="math notranslate nohighlight">
\[\text{leaky_relu}(x) := \max(\varepsilon x,x) \text{ with } \varepsilon \ll 1\]</div>
<p>Namely Leaky ReLU is weakly linear for <span class="math notranslate nohighlight">\(x &lt; 0\)</span>, instead of being 0 like ReLU.</p>
<div class="figure align-center" id="id5">
<img alt="leaky_relu" src="images/LeakyReLU.svg" /><p class="caption"><span class="caption-text">Figure 5: Leaky Rectified Linear Unit</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">LeakyReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<p>The Softmax function is used in the output layer of the network to yield a probability
distribution over the different classes, e.g. when trying to classify whether
there is a cat or dog in the picture we would like to know how certain the network
is about its prediction. That is we would like some output like:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\text{&quot;dog&quot;}) = 80 \%\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\text{&quot;cat&quot;}) = 20 \%\)</span></p></li>
</ul>
<p>This type of prediction can be achieved with the formula</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x_i) := \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}.\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">Softmax</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Activation functions can also be added to dense layers. If no activation function is
specified, the layer uses the Sigmoid function. E.g. you can add ReLU to a dense layer via</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">ReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">denseLayer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>Now that we have established how the output of a neural network can be computed,
we need to define a minimization problem, such that we can tweak the parameters of
our neural network. For that purpose, we need the so called loss function which
we are minimizing. The loss function is supposed to be an indicator for the quality
of the neural network and depending on the function we choose, we get different results.</p>
<p>In our framework we have implemented the loss functions:</p>
<div class="section" id="mean-squared-error">
<h3>Mean Squared Error<a class="headerlink" href="#mean-squared-error" title="Permalink to this headline">¶</a></h3>
<p>The Mean Squared Error (MSE) describes the <span class="math notranslate nohighlight">\(L^2\)</span>-error between the predictions of
the neural network and the expected outputs.</p>
<div class="math notranslate nohighlight">
\[\text{MSE}(X) := \frac{1}{2|X|} \sum_{x \in X} ||\text{NN}(x) - y(x)||^2\]</div>
<p>This kind of loss function can be used for regression tasks and we will for example use the Mean Squared Error when training denoising autoencoders.</p>
</div>
<div class="section" id="crossentropy">
<h3>Crossentropy<a class="headerlink" href="#crossentropy" title="Permalink to this headline">¶</a></h3>
<p>Given that the output of the neural network resembles a probability distribution, i.e. all output values are between 0 and 1,
then we can use the Cross Entropy loss function. This is often being used for object classification tasks, since then a binary classifier is being trained for each object class.
Hence all output values are between 0 and 1, where 0 means “doesn’t belong to the class” and 1 stands for “belongs to the class”.
Thus we can use Crossentropy for our loss, since the output of the neural network is being forced to either be close to 0 or close to 1.</p>
<div class="math notranslate nohighlight">
\[\text{Crossentropy}(X) := -\frac{1}{|X|} \sum_{x \in X} \Big[ y(x) \ln \left(\text{NN}(x) \right) + (1 - y(x)) \ln \left(1 - \text{NN}(x) \right) \Big]\]</div>
</div>
</div>
<div class="section" id="optimizers">
<h2>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h2>
<p>TO DO</p>
</div>
<div class="section" id="code-training-a-classifier">
<h2>Code: Training a Classifier<a class="headerlink" href="#code-training-a-classifier" title="Permalink to this headline">¶</a></h2>
<p>First we need to import all of our custom classes, which are needed to create a classifier.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nn</span> <span class="kn">import</span> <span class="n">MLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">activations</span> <span class="kn">import</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">Softmax</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">loss</span> <span class="kn">import</span> <span class="n">CrossEntropy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">dataset</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">optimizer</span> <span class="kn">import</span> <span class="n">Adam</span>
</pre></div>
</div>
<p>Then we import the MNIST dataset which contains grayscale images of shape 28x28.
It consists of 60,000 training samples and 10,000 test samples.
We chose a batch size of 50.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;mnist&quot;</span><span class="p">,</span> <span class="n">train_size</span> <span class="o">=</span> <span class="mi">60000</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<p>Our classifier is a Multi-Layer Perceptron with 784 neurons in the input layer, 100 neurons and 50 neurons in the hidden layer,
as well as 10 neurons in the output layer. We need 10 output neurons, since the classes of the digits have been one-hot encoded, i.e.
<span class="math notranslate nohighlight">\(\scriptsize 0\ \hat{=}\ \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}^T\)</span>,
<span class="math notranslate nohighlight">\(\scriptsize 1\ \hat{=}\ \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}^T\)</span>, etc.
In most layers we use the ReLU activation function and in the output layer we use the Softmax function, since it outputs a probability distribution over the digits.
All weights are being trained with the Adam optimizer, since it performed better than Stochastic Gradient Descent.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">inputDim</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">outputDim</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(),</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
<p>Finally, we can start our training loop in which we are tring to minimize the Crossentropy loss.
We are using 10 epochs, which means that the classifier is being trained on the entire training set 10 times.
We also monitor these losses on the training and test set. Additionally, we log the predicition accuracy on the training and test set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="p">(),</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="s2">&quot;train_accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;test_accuracy&quot;</span><span class="p">],</span> <span class="n">tensorboard</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">{})</span>
</pre></div>
</div>
<p>After training this classifier, which has been created with less than 15 lines of code, we can correctly predict handwritten digits in more than 97 % of the cases.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="classifier_demo.html" class="btn btn-neutral float-right" title="Multi-Layer Perceptron: Classification of handwritten digits (MNIST)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting started: Overview &amp; Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Julian Roth, Max Schröder, Annika Heil, Yerso Checya Sinti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>